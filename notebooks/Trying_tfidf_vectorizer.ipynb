{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('c:/Users/Owen/books_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5111046</td>\n",
       "      <td>A38UEYF5AFDU4I</td>\n",
       "      <td>1416583475</td>\n",
       "      <td>Jason Joyner \"On the journey\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Overview: What would a Mexican housekeeper, a ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fiction with Themes that Matter</td>\n",
       "      <td>1272326400</td>\n",
       "      <td>04 27, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8287159</td>\n",
       "      <td>A3RGUSBU339O2S</td>\n",
       "      <td>B00CPSGLEE</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I began reading as soon as I downloaded and di...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fast Paced!</td>\n",
       "      <td>1381968000</td>\n",
       "      <td>10 17, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6903249</td>\n",
       "      <td>A194K60CWZ371J</td>\n",
       "      <td>1607746336</td>\n",
       "      <td>Ellybean</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>The Mix &amp; Match Guide to Companion Planting is...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Useful for Any Level Gardener!</td>\n",
       "      <td>1403654400</td>\n",
       "      <td>06 25, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5982295</td>\n",
       "      <td>ARMNRH4HV1W4B</td>\n",
       "      <td>148118928X</td>\n",
       "      <td>Miss g</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>It isn't often I have an opportunity to read a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The Carpenter - Review</td>\n",
       "      <td>1364515200</td>\n",
       "      <td>03 29, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1752754</td>\n",
       "      <td>A3IOBVF6E8SPRL</td>\n",
       "      <td>0373295987</td>\n",
       "      <td>Sue</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I love this book. Very interesting story, char...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Carla Kelly is great, and this is one of her b...</td>\n",
       "      <td>1391558400</td>\n",
       "      <td>02 5, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      reviewerID        asin                   reviewerName  \\\n",
       "0     5111046  A38UEYF5AFDU4I  1416583475  Jason Joyner \"On the journey\"   \n",
       "1     8287159  A3RGUSBU339O2S  B00CPSGLEE                Amazon Customer   \n",
       "2     6903249  A194K60CWZ371J  1607746336                       Ellybean   \n",
       "3     5982295   ARMNRH4HV1W4B  148118928X                         Miss g   \n",
       "4     1752754  A3IOBVF6E8SPRL  0373295987                            Sue   \n",
       "\n",
       "  helpful                                         reviewText  overall  \\\n",
       "0  [0, 0]  Overview: What would a Mexican housekeeper, a ...      5.0   \n",
       "1  [0, 0]  I began reading as soon as I downloaded and di...      4.0   \n",
       "2  [0, 0]  The Mix & Match Guide to Companion Planting is...      5.0   \n",
       "3  [2, 2]  It isn't often I have an opportunity to read a...      4.0   \n",
       "4  [0, 0]  I love this book. Very interesting story, char...      5.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                    Fiction with Themes that Matter      1272326400   \n",
       "1                                        Fast Paced!      1381968000   \n",
       "2                     Useful for Any Level Gardener!      1403654400   \n",
       "3                             The Carpenter - Review      1364515200   \n",
       "4  Carla Kelly is great, and this is one of her b...      1391558400   \n",
       "\n",
       "    reviewTime  \n",
       "0  04 27, 2010  \n",
       "1  10 17, 2013  \n",
       "2  06 25, 2014  \n",
       "3  03 29, 2013  \n",
       "4   02 5, 2014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority class baseline accuracy:  0.559028\n"
     ]
    }
   ],
   "source": [
    "# Get majority class baseline.\n",
    "target = 'overall'\n",
    "majority_class = df[target].mode()[0]\n",
    "baseline_guess = [majority_class] * len(df)\n",
    "print('Majority class baseline accuracy: ', accuracy_score(df[target], baseline_guess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 60 missing reviews. \n",
    "df = df.dropna(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create punctuation list.\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create stopwords list.\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load tokenizer, tagger, parser, NER, and word vectors.\n",
    "parser = English()\n",
    "\n",
    "# Create tokenizer function.\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Create token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # For each token, lemmatize and change to lowercase.\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-'\n",
    "                else word.lower_ for word in mytokens]\n",
    "    # Remove stop words.\n",
    "    mytokens = [word for word in mytokens if word not in stop_words\n",
    "                and word not in punctuations]\n",
    "    # Return preprocessed list of tokens.\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer) # Don't think I need this line.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features matrix and target vector.\n",
    "X = df['reviewText']\n",
    "y = df[target]\n",
    "\n",
    "# Randomly split data into stratified train (80%) and test (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x00000296CA4F5C08>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x00000297C6448708>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiate the classifier with mostly default hyperparameters.\n",
    "classifier = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1,\n",
    "                                random_state=42)\n",
    "\n",
    "# Create pipeline to clean, get bag-of-words review vectors, then run logistic\n",
    "# regression.\n",
    "pipe = Pipeline([('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit the model on train.\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6331979918795128\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create predictions vector from test features.\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Find model accuracy.\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit for most of the nlp with spacy and custom transformer to Avinash Navlani. \n",
    "# https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features matrix and target vector.\n",
    "X = df['reviewText']\n",
    "y = df[target]\n",
    "\n",
    "# Randomly split data into stratified train (80%) and test (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_vecs = bow_vector.fit_transform(X_train)\n",
    "X_test_vecs = bow_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((799952, 1168155),\n",
       " <199988x1168155 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 10444295 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vecs.shape, X_test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_test_vecs, y_test)]\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softmax', \n",
    "    num_class=5,  \n",
    "    seed=42,\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train_vecs, y_train, eval_set=eval_set, \n",
    "          eval_metric='merror', early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# class_weights = {\n",
    "#     5.0:0.559019,\n",
    "#     4.0:0.250450,\n",
    "#     3.0:0.107219,\n",
    "#     2.0:0.046716,\n",
    "#     1.0:0.036596\n",
    "# }\n",
    "\n",
    "# # Instantiate and set hyperparameters for Gradient Boosting Decision Tree Classifier.\n",
    "# classifier = LGBMClassifier(objective='multiclass', num_class=5, \n",
    "#                             metric='multi_error', class_weight=class_weights,\n",
    "#                             num_jobs=-1, \n",
    "#                             seed=42)\n",
    "# Gives error:\n",
    "# OSError: exception: access violation reading 0x00000000000002A8\n",
    "\n",
    "# # Create pipeline using Bag of Words. \n",
    "# pipe = make_pipeline(predictors(),\n",
    "#                      bow_vector,\n",
    "#                      classifier)\n",
    "\n",
    "# # Fit the model on training data.\n",
    "# pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Errors',\n",
       " 'GlobalRegistry',\n",
       " 'Model',\n",
       " 'OrderedDict',\n",
       " 'Vectors',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__test__',\n",
       " 'basestring_',\n",
       " 'functools',\n",
       " 'get_array_module',\n",
       " 'get_string_id',\n",
       " 'numpy',\n",
       " 'path2str',\n",
       " 'srsly',\n",
       " 'unpickle_vectors',\n",
       " 'util']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spacy.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    0.559019\n",
       "4.0    0.250450\n",
       "3.0    0.107219\n",
       "2.0    0.046716\n",
       "1.0    0.036596\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.distplot(df[target], kde=False)\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# 1) Make multiple line graph of the number of each rating over the years.\n",
    "# Like are people giving more 5s as time passes? See if the slope of each line\n",
    "# is the same. Do I need to do time-based split?\n",
    "\n",
    "# 2) In your model, set class_balance/class weights.\n",
    "\n",
    "# 3) Figure out how leakage appllies to your data.\n",
    "# Answer: a) Avoid leaky predictors (target leakage) - drop 'helpful' feature\n",
    "#         b) Avoid leaky validation strategies - make sure you include preprocessing in the pipeline.\n",
    "\n",
    "# 4) Figure out which accuracy metric to use.\n",
    "# Answer: 'merror'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
