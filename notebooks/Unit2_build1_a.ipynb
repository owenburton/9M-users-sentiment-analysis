{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one million book reviews.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('c:/Users/Owen/books_small.csv')\n",
    "\n",
    "# Drop missing reviews.\n",
    "df = df.dropna(subset=['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "# Create punctuation list.\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create stopwords list.\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load tokenizer, tagger, parser, NER, and word vectors.\n",
    "parser = English()\n",
    "\n",
    "# Create tokenizer function.\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Create token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # For each token, lemmatize and change to lowercase.\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-'\n",
    "                else word.lower_ for word in mytokens]\n",
    "    # Remove stop words.\n",
    "    mytokens = [word for word in mytokens if word not in stop_words\n",
    "                and word not in punctuations]\n",
    "    # Return preprocessed list of tokens.\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of original df so you don't have to reload the original every new experiment.\n",
    "copy = df.copy()\n",
    "\n",
    "feature = 'reviewText'\n",
    "target = 'overall'\n",
    "\n",
    "copy = copy[['reviewText', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.80892\n",
       "0.0    0.19108\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_copy = copy.sample(100000, random_state=1)\n",
    "small_copy = small_copy.replace({5.0:1.0, 4.0:1.0, 3.0:0.0, 2.0:0.0, 1.0:0.0})\n",
    "small_copy.overall.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = small_copy[feature]\n",
    "y = small_copy[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(tokenizer = spacy_tokenizer,\n",
    "                            ngram_range = (1,1),\n",
    "                            max_df = 0.5,\n",
    "                            min_df = 2)\n",
    "\n",
    "X_train_vecs = tfidf_vec.fit_transform(X_train)\n",
    "X_test_vecs = tfidf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 69425), (20000, 69425))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vecs.shape, X_test_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:  2.8min remaining: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  15 | elapsed:  2.8min remaining: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:  2.8min remaining:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  15 | elapsed:  2.8min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  2.8min remaining:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  15 | elapsed:  2.8min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  2.8min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  15 | elapsed:  5.4min remaining:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:  5.4min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  15 | elapsed:  5.4min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  5.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  15 | elapsed:  5.4min remaining:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.4min finished\n",
      "C:\\Users\\Owen\\Anaconda3\\envs\\unit2\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "# n_estimators for final model: 1571\n",
    "model = LGBMClassifier(objective='binary', is_unbalance=True, num_jobs=-1, seed=0)\n",
    "\n",
    "params = { \n",
    "    'model__num_leaves': randint(1, 1000), \n",
    "    'model__max_bin': randint(254, 1000),  \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    model, \n",
    "    param_distributions=params, \n",
    "    n_iter=5, \n",
    "    cv=3, \n",
    "    scoring='f1', \n",
    "    verbose=30, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train_vecs, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'model__max_bin': 696, 'model__num_leaves': 993}\n",
      "Cross-validation Accuracy -0.8685522721018395\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation Accuracy', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.80555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = search.predict(X_test_vecs)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.80555\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_final = LGBMClassifier(objective='binary', is_unbalance=True, n_iterations=1571, \n",
    "                             max_bin=696, num_leaves=993, num_jobs=-1, seed=0)\n",
    "\n",
    "model_final.fit(X_train_vecs, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8686\n"
     ]
    }
   ],
   "source": [
    "y_preds = model_final.predict(X_test_vecs)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8713\n",
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final = copy.sample(10000, random_state=42)\n",
    "final = final.replace({5.0:1.0, 4.0:1.0, 3.0:0.0, 2.0:0.0, 1.0:0.0})\n",
    "X = final[feature]\n",
    "y = final[target]\n",
    "X_final = tfidf_vec.transform(X)\n",
    "y_final = model_final.predict(X_final)\n",
    "print(\"Test Accuracy:\", accuracy_score(y, y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickle stuff.\n",
    "from joblib import dump\n",
    "dump(model_final, 'model.joblib', compress=True)\n",
    "dump(tfidf_vec, 'vectorizer.joblib', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "Building wheels for collected packages: dill\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78598 sha256=9fcd2d6f1d6d8e6bce40d9b401338cd465b381a09082eed5c4bc02cef54c9964\n",
      "  Stored in directory: C:\\Users\\Owen\\AppData\\Local\\pip\\Cache\\wheels\\59\\b1\\91\\f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "Successfully built dill\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.sample(10000, random_state=42)\n",
    "copy = copy.replace({5.0:1.0, 4.0:1.0, 3.0:0.0, 2.0:0.0, 1.0:0.0})\n",
    "copy.to_csv('10k_books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('foobar.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joblib==0.14.0\n",
      "scikit-learn==0.21.3\n",
      "lightgbm==2.3.0\n",
      "spacy==2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Get versions of packages used.\n",
    "import joblib\n",
    "import sklearn\n",
    "import lightgbm\n",
    "import spacy\n",
    "print(f'joblib=={joblib.__version__}')\n",
    "print(f'scikit-learn=={sklearn.__version__}')\n",
    "print(f'lightgbm=={lightgbm.__version__}')\n",
    "print(f'spacy=={spacy.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'string' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-133fc45372c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'string=={string.__version__}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'string' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(f'string=={string.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
